#!/usr/bin/env python3
"""
Test Backward Compatibility

This script verifies that the new context-aware evaluation system maintains
backward compatibility with existing test suites and doesn't break current functionality.
"""
import asyncio
from typing import Dict, Any
from empathetic.core.tester import Tester
from empathetic.tests.bias import BiasTests
from empathetic.tests.safety import SafetyTests
from empathetic.tests.empathy import EmpathyTests
from empathetic.evaluation.unified_evaluator import UnifiedEvaluator


class BackwardCompatibilityTester:
    """Tests backward compatibility of the enhanced evaluation system"""
    
    def __init__(self):
        self.test_suites = {
            "bias": BiasTests(),
            "safety": SafetyTests(),
            "empathy": EmpathyTests()
        }
        self.evaluator = UnifiedEvaluator()
    
    async def test_existing_api(self):
        """Test that existing API still works"""
        print("\n=== Testing Existing API ===")
        
        # Test basic tester functionality
        tester = Tester()
        
        # Mock provider for testing
        class MockProvider:
            async def generate(self, prompt, **kwargs):
                return type('Response', (), {'content': 'Mock response for testing'})()
            
            async def detect_capabilities(self):
                return {
                    "context_length": 4096,
                    "supports_system_prompt": True,
                    "supports_json_output": False
                }
        
        # Test that provider creation works
        provider = MockProvider()
        print("âœ… Provider creation works")
        
        # Test that test suites can be instantiated
        for name, suite in self.test_suites.items():
            assert suite is not None
            assert hasattr(suite, 'run')
            assert hasattr(suite, 'test_cases')
            print(f"âœ… {name.capitalize()} test suite instantiation works")
        
        return True
    
    async def test_test_case_structure(self):
        """Verify test case structure hasn't changed"""
        print("\n=== Testing Test Case Structure ===")
        
        # Check bias test cases
        bias_tests = self.test_suites["bias"]
        if bias_tests.test_cases:
            test_case = bias_tests.test_cases[0]
            
            # Verify required attributes
            required_attrs = ['id', 'category', 'input', 'expected_behavior', 
                            'harmful_patterns', 'severity']
            
            for attr in required_attrs:
                assert hasattr(test_case, attr), f"Missing attribute: {attr}"
            
            print("âœ… Test case structure is intact")
        
        return True
    
    async def test_evaluation_methods(self):
        """Test that evaluation methods still work"""
        print("\n=== Testing Evaluation Methods ===")
        
        # Test pattern matching (original method)
        from empathetic.utils.context_analyzer import ContextAnalyzer
        
        analyzer = ContextAnalyzer()
        text = "The nurse walked into the room. She was tired."
        pattern = "she"
        
        # Test original method
        matches = analyzer.analyze_pattern_context(text, pattern)
        assert isinstance(matches, list)
        print("âœ… Original pattern matching works")
        
        # Test should_pattern_fail_test method
        should_fail, reason = analyzer.should_pattern_fail_test(matches)
        assert isinstance(should_fail, bool)
        assert isinstance(reason, str)
        print("âœ… Pattern failure detection works")
        
        return True
    
    async def test_enhanced_evaluation_optional(self):
        """Verify enhanced evaluation is optional"""
        print("\n=== Testing Enhanced Evaluation is Optional ===")
        
        # Test without enhanced evaluation
        from empathetic.tests.bias import BiasTests
        bias_tests = BiasTests()
        
        # Mock provider
        class MockProvider:
            async def generate(self, prompt, **kwargs):
                return type('Response', (), {
                    'content': 'A typical nurse is caring and compassionate.'
                })()
        
        provider = MockProvider()
        
        # Should work with original evaluation
        test_case = bias_tests.test_cases[0] if bias_tests.test_cases else None
        if test_case:
            response = await provider.generate(test_case.input)
            passed = bias_tests.evaluate_response(response.content, test_case)
            assert isinstance(passed, bool)
            print("âœ… Original evaluation method works")
        
        return True
    
    async def test_api_endpoints(self):
        """Test API endpoint compatibility"""
        print("\n=== Testing API Endpoint Compatibility ===")
        
        # Check that new endpoints don't break existing ones
        from empathetic.api.models.testing import TestRequest, TestResult
        
        # Original request model should still work
        request = TestRequest(
            model="gpt-4",
            suites=["bias", "empathy"],
            quick_mode=False,
            enable_validation=False
        )
        
        assert request.model == "gpt-4"
        assert request.suites == ["bias", "empathy"]
        print("âœ… Original TestRequest model works")
        
        # New field should be optional with default
        assert hasattr(request, 'enhanced_evaluation')
        assert request.enhanced_evaluation == False  # Default value
        print("âœ… New field is optional with correct default")
        
        # Test with new field
        request_enhanced = TestRequest(
            model="gpt-4",
            suites=["bias"],
            enhanced_evaluation=True
        )
        assert request_enhanced.enhanced_evaluation == True
        print("âœ… New field can be set explicitly")
        
        return True
    
    async def test_metrics_compatibility(self):
        """Test metrics calculation compatibility"""
        print("\n=== Testing Metrics Compatibility ===")
        
        from empathetic.metrics.calculators import MetricsCalculator
        
        # Test that original metrics still work
        calculator = MetricsCalculator()
        
        # Mock test results
        test_results = {
            "bias": {"score": 0.85, "tests_passed": 17, "tests_total": 20},
            "empathy": {"score": 0.90, "tests_passed": 18, "tests_total": 20}
        }
        
        # Calculate overall score
        weights = {"bias": 0.5, "empathy": 0.5}
        overall_score = sum(
            test_results[suite]["score"] * weights.get(suite, 0.25)
            for suite in test_results
        )
        
        assert 0 <= overall_score <= 1
        print("âœ… Metrics calculation works")
        
        return True
    
    async def run_all_tests(self):
        """Run all backward compatibility tests"""
        print("=" * 60)
        print("Running Backward Compatibility Tests")
        print("=" * 60)
        
        tests = [
            ("Existing API", self.test_existing_api),
            ("Test Case Structure", self.test_test_case_structure),
            ("Evaluation Methods", self.test_evaluation_methods),
            ("Enhanced Evaluation Optional", self.test_enhanced_evaluation_optional),
            ("API Endpoints", self.test_api_endpoints),
            ("Metrics Compatibility", self.test_metrics_compatibility)
        ]
        
        results = {}
        
        for test_name, test_func in tests:
            try:
                result = await test_func()
                results[test_name] = "PASSED" if result else "FAILED"
            except Exception as e:
                results[test_name] = f"FAILED: {str(e)}"
                print(f"âŒ {test_name} failed: {e}")
        
        # Summary
        print("\n" + "=" * 60)
        print("Backward Compatibility Test Summary")
        print("=" * 60)
        
        all_passed = True
        for test_name, result in results.items():
            status = "âœ…" if result == "PASSED" else "âŒ"
            print(f"{status} {test_name}: {result}")
            if result != "PASSED":
                all_passed = False
        
        print("\n" + "=" * 60)
        if all_passed:
            print("âœ… All backward compatibility tests PASSED!")
            print("\nThe enhanced evaluation system maintains full backward")
            print("compatibility with existing test suites and APIs.")
        else:
            print("âŒ Some tests failed. Please review the issues above.")
        
        return all_passed


async def main():
    """Run backward compatibility tests"""
    tester = BackwardCompatibilityTester()
    success = await tester.run_all_tests()
    
    if success:
        print("\nðŸŽ‰ The context-aware evaluation system is fully backward compatible!")
        print("\nKey Points:")
        print("â€¢ All existing test suites continue to work")
        print("â€¢ The original API remains unchanged")  
        print("â€¢ Enhanced evaluation is opt-in via 'enhanced_evaluation' flag")
        print("â€¢ No breaking changes to test case structure")
        print("â€¢ Metrics calculation remains consistent")
    else:
        print("\nâš ï¸  Please fix compatibility issues before deployment")


if __name__ == "__main__":
    asyncio.run(main())